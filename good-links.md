---
layout: page
title: My Notes on Some Good Posts
permalink: /notes-on-good-posts/
---


Here is a list of links to insights that I found really interesting. I will continue to add more, both old and new (will limit it to under 100 links to prioritize what was most meaningful)

## 1. Metrics

### 1.1. General

1.1.1. [Don't Let Your North Star Metric Deceive You](https://brianbalfour.com/essays/north-star-metric-growth) - A great critique of the limitations of OMTM. Specifically it points out that North Star metrics are 1)Output metrics: too big, broad, not actionable. It's like a scoreboard for a game. (My wording: it just states the obvious in numerical terms, without any insight on what needs to be done) 2)Output metrics are lagging indicators. For instance, if we only pay attention to revenue and not product usage frequency by paying customers, we will only become aware of product problems when users churn at the end of their subscription period. 3)It only captures one dimension of the business,leading to over-optimization at the expense of other dimensions. It doesn't account for trade-offs, reminiscent of the Instrumental Convergence/Paperclip Fallacy. For example, monetization may be pursued at the expense of retention, or exploitation (producing yet another movie sequel) at the expense of exploration (creating something novel and refreshing). One critique I have of this blog post is that while I appreciate it drawing a distinction between input and output metrics, some of the input metrics are only inputs with respect to the final output metric-they are not inputs i.e. actionable from the business perspective. Metrics such as "Bring users back more often" are intermediate metrics between actual business levers or inputs and desired final output.

1.1.2. [Don’t Become a Victim of One Key Metric](https://caseyaccidental.com/one-key-metric-victim) -  An example here reminded me of the Munger quote, 'Show me the incentive and I'll show you the outcome.' Grubhub and Seamless had similar businesses with very different key metrics. Grubhub optimized for revenue, while Seamless optimized for GMV. Consequently, Seamless would show restaurants in alphabetical order in their search results, while Grubhub sorted restaurants by the average commission they earned from orders. Another example discussed how Pinterest's metric ended up incentivizing the algorithm to show clickbait images, with the comment, 'The metrics make it appear that engagement is increasing. It might be, but it is an empty calorie form that will affect engagement in a very negative way over the long term.' This made me reflect on how I try to stay away from Instagram after consuming enough 'empty calories' there, leading to a step function change in my usage after a period of high activity.

1.1.3. [Common Mistakes In Defining Metrics](https://brianbalfour.com/quick-takes/common-mistakes-defining-metrics) - This is where I first read about and realized engagement is depth and retention is binary. I keep thinking of this article ever so often and so this definitely needed a spot in this list, since I know I will be looking for it in future.

1.1.4. [Bezos on managing to metrics](https://www.youtube.com/watch?v=8ij964FCQiw) - Keeping this here as a note to remember Goodhart's Law-even though it is a familiar concept, it's easy to fall for it unless vigilant.

1.1.5. [What Moneyball-for-Everything Has Done to American Culture](https://www.theatlantic.com/newsletters/archive/2022/10/sabermetrics-analytics-ruined-baseball-sports-music-film/671924/) - Argues that the analytics revoultion has approached sports,music and movies like an equation and "solved" it-in the process making them more homogenous. It sacrifices exuberance for the sake of formulaic symmetry. It sacrifices diversity for the sake of familiarity. This is interesting-while I have definitely observed what the article argues, at the same time we can also observe how the internet has created many subcultures and niche interests. I am not sure how to tie these contradictory observations, but it is something worth chewing on.


### 1.2. Growth/Product 

1.2.1. [Your Average CAC is Lying to You -- What to do Instead](https://brianbalfour.com/essays/average-cac-mistakes-growth) - Talks about the importance of looking at segmented CAC vs LTV instead of average CAC vs LTV. Segmentation may be based on Customer Type (e.g. Tier), Channel (e.g. FB, Twitter, Google), Geography etc. Additionally CAC changes over time-channel based evolution or network-effect evolution may reduce CAC over time whereas saturation-based evolution increases CAC.

1.2.2. [How To (Actually) Calculate CAC](https://andrewchen.com/how-to-actually-calculate-cac/) - Makes a distinction between CAC (acquiring customers) and CPA (acquiring leading indicators to paying customers). Makes the point that CAC should be based on length of sales cycle, distinguish between new vs. returning customers and be fully loaded i.e. account for all the sales and marketing salaries, overhead costs. My question here though is how often might length of sales cycle change? How often do we check for it? What would be the implication of longer CAC maturity?

1.2.3. [A Deep Dive Into User Engagement Through Tricky Averages](https://dataanalysis.substack.com/p/a-deep-dive-into-user-engagement) - I appreciate this article for putting into writing and serving as a reminder for a common problem anyone working with data has experienced/will continue to experience. Essentially, the denominator that is chosen to calculate per user averages can change the story being told. The story changes because the user segments (and therefore user behavior) captured changes with the choice of denominator. It is a reminder to have more clarity and specificity on what we want to know. The right method of average per user metric depends on the question being asked.

1.2.4. [Building your user engagement pyramid](https://versionone.vc/engagement-pyramid/)-This is an interesting way to think of engagement: consider all the other activities that someone can do, and place them in decreasing order from what is the hardest / has the most friction for someone to do / requires the most energy, to what is easiest.

1.2.5. [Why Retention Is The Silent Killer](https://www.reforge.com/blog/retention-engagement-growth-silent-killer) - I appreciated the simple quantitative model illustrating the leaky bucket effect of retention i.e. even if Company B acquires 2x users of Company A, if retention of Company B is 20 pts lower than that of company A then company B ends up with fewer MAU after 3 years. The article also points out risks of incorrect retention definition. For instance choosing a MAU metrics for a B2C social product is misleading. For such products to retain users, they need to build a habit with users that is daily, not weekly or monthly. It gives the example of Viddy's DAU being a leading indicator of the dreaded "shark fin graph", that eventually showed up in the MAUs it was tracking. "What we learn from Viddy is that it’s possible for companies to manipulate their retention and top line metrics to look good, just by expanding the frequency." The article also mentions the value of engagement depth for driving revenue and creating defensibility (product mastery, organizational embedding and network effects in the latter case).

1.2.6. [Retention Benchmarks](https://brianbalfour.com/quick-takes/retention-benchmarks) - I am very wary of averages and blended metrics, and even so this article gave me yet another point against averages- Averages are useless.  You want to benchmark against best in class. Another interesting learning from this was the point around when low retention is ok-anything with low acquisition and marginal costs but power law returns. Additionally, I also liked the Gusto example on knowing what you can and cannot influence i.e. since Gusto cannot influence how many employees thier customers hire, they can only maintain their customers and no expand. What they can go after is greater wallet share but offering more product lines.

1.2.7. [Experiment Segmentation: Avoiding Old Dogs and Watered Down Results](https://jwegan.com/growth-hacking/experiment-segmentation-avoiding-old-dogs-watered-results/) - Another warning on the use of averages or blended metrics. This post talks about the importance of segmenting new vs. returning users. I have certainly learnt the importance of doing so, when working on a page redesign where new users reacted positively and returning users negatively and given the share of new vs. returning the overall effect was negative. The other example the post offers is the importance of cutting users by the engagement levels. For instance a push notification experiment at Pinterest yielded 3% overall lift, but on a segment level it actually yielded 10% for less engaged users and had no effect for core users. 

### 1.3. Financial Metrics

1.3.1 [Cash Conversion Cycle: Bridging the
Gap Between Profitability & Cash Flow](https://every.to/napkin-math/cash-conversion-cycle-bridging-the-18577406) - Points out how the income statement describes the profitability of a company but not the cash dynamics. A cash conversion cycle (similar to CAC payback period)  is the amount of time it takes for a company to return the cash investment in inventory back to the company. The timing is an important distinction because supplier relationships and therefore access to inventory is both determined and impacted by CCC. The article has a chart showing the CCC for eComm and retail companies at the time of their IPO: from -163.8 for Blue Nile to 152.5 for Canada Goose. It explains Blue Nile doesn’t procure diamonds from their supplier until a customer pays for them, and due to long-term contracts, Blue Nile doesn’t have to pay their suppliers for 30-120 (or more) days. For Canada Goose, they have to carry more inventory, making their DIO high, since the seasonal nature of their business makes it hard to predict demand. (Demand is easier to predict QoQ than YoY). This also mentions ways to improve CCC and makes the interesting note that Amazon is able to leverage their annual Amazon Prime Membership to drive a negative CCC (along with their negotiating power due to their vast scale).
[The Power Of Having A Negative CCC](https://jayvas.com/the-power-of-having-a-negative-cash-conversion-cycle/) from the additional reading section makes an interesting point about reducing inventory by “hold only the most popular SKUs at your warehouse. For unpopular SKUs, simply increase the delivery time for your customers. This will allow you to order inventory for unpopular SKUs, subsequent to receiving customer orders for it”


## 2. Formulas, Models, Dashboards:

2.1. [The One Growth Metric that Moves Acquisition, Monetization, and Virality](https://www.reforge.com/blog/growth-metric-acquisition-monetization-virality) - This blog has 3 simple quantitative models that are useful: 1)How Increasing Retention Improves Acquisition (new users from virality) 2) How various levels of retention impacts Monetization 3)How various levels of retention impacts payback period (Payback period is the time it takes to break even on your fully loaded CAC. This is the limiting factor because the payback period determines how much cash is needed to fuel growth of the business. If you have a longer payback period, you either need to raise more money to fuel acquisition or wait longer to reinvest in acquisition.). Additionally the article makes a point on how higher retention means higher LTV and so being able to afford higher CAC.

2.2. Questions and dashboard to model marketplaces: [The data we use to evaluate marketplace traction](https://versionone.vc/marketplace-traction-data/) and [Marketplace KPI Dashboard](https://versionone.vc/marketplace-kpi/)

2.3 [The 27 Metrics in Pinterest’s Internal Growth Dashboard](https://jwegan.com/growth-hacking/27-metrics-pinterests-internal-growth-dashboard/)


## 3. A/B Testing

3.1. [5 warning signs: Does A/B testing lead to crappy products?](https://andrewchen.com/does-ab-testing-lead-to-crappy-products/) - Keeping this article as a source of reference that clearly articulates common concerns around AB testing. The point on "Quitting too early" is one that has always concerned me. It especially applies when page redesigns are done and returning customers react badly to it. Questions arise: Do we wait for returning users to adjust to the changes or do we revert to how things were? What is sufficient time to allow behavior change? Is it worth accepting negative impact on revenue metrics while we hope for users to hit a steady state in their response? These are perhaps philosophical questions, in the same category as When is the right time to pivot a business? When is it the right time to give up on something? But these often creep into the scope of AB tests. 

3.2. [Are RCTs a good way to do economics](https://someunpleasant.substack.com/p/real-chaos-today?utm_source=post-email-title&publication_id=261003&post_id=59956889&utm_campaign=email-post-title&isFreemail=true&r=9hzj0&triedRedirect=true&utm_medium=email) - I found this interesting because the concerns it articulates around RCTs parallel the ones I have with A/B testing. For instance, that not everything is tractable to RCTs; there are concerns about external validity and whether RCTs produce generalizable knowledge; the bias-variance tradeoff for internal validity; and whether the average effect is the most relevant statistic (in contrast to measures of questions such as “who benefits the most”, “how much do group X and group Y benefit”, “over what timeframe are benefits fully accrued”) -these are all concerns that are very relevant to AB testing. Also, I found the concluding line "Ultimately, why projects work is, fundamentally, a better question than whether they work." quite thought-provoking. 

3.3. [Why Would I Ever Write a Growth Experiment Doc?](https://medium.com/@talraviv/why-would-i-ever-write-a-growth-experiment-doc-6ef2649fd215) - Good reference for test planning doc which should have 1)Hypothesis/Belief 2)Predicted Outcome based on Hypothesis/Belief in #1 3)Experiment Design: a)% users needed b)time needed for the experiment to run c)how risky is the experiment d)interaction with other experiments 4)Sufficient Test + Spec. Fight the feature creep.Dump the feature creep ideas in the backlog as a "mental hack we do to ourselves to keep scope small" 5)Instrumentation: what events and properties are needed; what platform will the data be in 6)Deep Dives: “ideas for segmentations and analysis ahead of time to remind our future-selves of ways to dig in deeper" 7)Learnings: Was our belief totally invalid? How much were our business metrics actually impacted? 8)Next Steps: are we going deeper in this route or trying other routes? 9)Knowledge share with other teams and future self. Notably "Two years from now, when 'future you' thinks of the same idea you disproved today, you’ll be able to dig in and know exactly what you did and what you didn’t try"

3.4. [That’s Not a Hypothesis](https://medium.com/@talraviv/thats-not-a-hypothesis-25666b01d5b4) - A hypothesis is a belief that is falsifiable. It leads to prediction(s). Hypotheses may come from reasons (past experiments, user research, conversation with sales, particular metric) or per Feynman it could be a guess. (My takeaway: good hypothesis is deeper than a prediction and it explains "why"). Difference b/w prediction and hypothesis: If all you have is “If I put my plant in the dark it will die” (a prediction) and that turns out to be false, then all you learned is… that sentence was wrong. Testing hypotheses should refine mental model of users: How do they think? What do they care about? How do they behave? What don’t they know?. Sometimes mental models need an overhaul, a paradigmatic shift. Good hypotheses cite sources. Note the Problem -> Hypothesis -> Prediction example using Freemind is great: simple, small scope, explains exactly why we are doing what we are doing.

3.5. [Challenges in Experimentation](https://eng.lyft.com/challenges-in-experimentation-be9ab98a7ef4) - Mentions region split test where "we use trends from before the test begins to develop counterfactual predictions of what would have happened had we not launched a treatment"-My question: how is this different from diff-in-diff or event studies analysis? Also mentions time-split variance reductions techniques like residualization (does not elaborate; not sure what this is but may be interesting). The article calls out the challenges of managing real world dynamism i.e. from weather to macroeconomic trends to expanding business lines. I would also add changes in customer behavior due to changes in culture e.g. let's say people opt to use more public transit or changes in competitive landscape e.g. Waymo rolling out self driving cars. "Together, these forces mean our customers’ behavior can vary widely across time and space while changing rapidly from month to month. Experimental results may lose external validity over time. Parameters that were tested and set years ago may no longer be rational. Updating these parameters requires time consuming time-split tests and the results may not even hold for long!". One answer, per the article is to use adaptive experimentation platform-testing widely and quickly. I need to look into details of what parameter tuning and contextual bandits achieve. But my greater concern remains over how quickly test results become stale, the reflexive nature of A/B tests, the impact of the order in which A/B tests are run. Next, the article mentions using randomization units beyond the standard sessions, riders, drivers to region splits and hardware splits. The article also shares 3 common pitfalls in experimentation. One in HARKING i.e. fitting a post hoc hypothesis to the data which results in lack of generalizability due to false positives. Second is the multiple comparisons problem, where statistically the chance of false positives increases as the number of comparisons increases. They use Benjamini-Hochberg method to build Multiple Hypothesis Testing correction. I am unfamiliar with this-have only used Bonferroni correction in the past. The third pitfall is unaligned tradeoffs. For instance, experiments where one team is offering say $10 coupon to acquire 1 ride and another team is obtaining $5 at the cost of losing 1 ride, but the net effect is that Lyft is losing $5 with no change in rides. 

3.6 [Please, Please Don’t A/B Test That](https://medium.com/@talraviv/please-please-dont-a-b-test-that-980a9630e4fb) - A/B testing can be expensive. There is time needed to plan-set up eligibility criteria, figure out a hypothesis. There is time needed to decide-cleaning data, digging into segments, and solidifying analysis. There is complexity in keeping two versions of the code. There are delays in customers experiencing the product. This article argues that given these costs, A/B testing should not be the default option. In fact A/B testing is useless when the outcome doesn’t matter. The change is something that will be implemented anyway. It is important to map outcomes to different actions. The A/B test is useless when we are dealing with a vanity metric that does not tie top level metrics (or guardrail/tradeoff metrics). Sometimes we can fall into streetlight fallacy (i.e. we measure what we can, not what we need to). The article does acknowledge that A/B testing is useful when there is need for precision-for instance, the need to learn what exactly works or what exactly are the tradeoffs: is the benefit worth the cost. A/B test is also important when working on high risk or high traffic areas such as payments where even a small chance of a bad thing will have large negative repercussions. 

## 4. Multivariate Testing

## 5. Instrumentation/Tracking

## 6. Growth

## 7. Product

7.1. [Tearing down the UX teardown](https://medium.com/@talraviv/tearing-down-the-teardown-incl-personal-example-2d8c5ce2dce6) - Good questions to keep in mind when analyzing specific sections of a user journey: 1)What do we believe about the user’s mindset? 2)What do we believe about the user setting? 3)how did the user get to this section? 4)what does the user already know? 5)How much motivation does the user have? 6)Who is the business focusing on vs. not focusing on here? 7)What is success in this part of the funnel? 8)Is there a part of the funnel we don’t see eg a sales component? The sample teardown at the end is interesting. It describes 1)The ideal state 2)What might thwart the ideal state 3)Non hypotheses (i.e. possible hypotheses but out of scope for our purposes) 4)Prioritization 5)3 test proposals + what to measure + what success looks like 

### 7.1 Onboarding

## 8. Marketplace/Platforms/Aggregators

8.1. [Developing a growth model + marketplace growth strategy](https://www.youtube.com/watch?v=AlTQ6O2qooI) - Some takeaways from this: 1)Retention is likely the most important growth metric 2)Retention is the hardest to move but core product levers such as depth of supply, interaction with supply matters more than growth levers like notificaitons 3)Early lifecycle experience is a good indicator of future retention (and so early intervention is more effective than when customers are about to churn). On a separate note, a common analytical failure mode: "Our best users do x, why can't we make others do the same". This generally does not work-the others have different intent/context. The goal would be to figure out where the value for "the others" lie. Regarding marketplace expansion, might make more sense to after adjacent/easier/network effect accentuating space compared to higher TAM. Also, product i.e. incredible E2E experience (atomic networks?) matters more than GTM investments in liquidity (@45:30). 

8.2. [Shopify Lowers Developer Fees, Scaling Digital Suppliers, The Power of App Stores](https://stratechery.com/2021/shopify-lowers-developer-fees-scaling-digital-suppliers-the-power-of-app-stores/) - 1)”The incentives for the platform owner are to get the most developers possible, not only to improve the attractiveness of the platform to end users (which in the case of Shopify are store owners), but also to dilute the power of and dependence on any one developer.” 2)”Shopify, on the other hand, does have competition, both from other SaaS ecommerce platforms like BigCommerce and also open source platforms like WooCommerce. It is very much in the company’s interest that developers spend their time on Shopify, not elsewhere, and by the same token, once a developer is making $1 million a year it is unlikely that they will go elsewhere. This dynamic, of making it easy and attractive to get started, such that you greatly increase the number of those giving your platform a try, and then making money off of those that succeed, characterizes the seller side of Shopify’s business as well.” 

8.3. [Shopify and the Power of Platforms](https://stratechery.com/2019/shopify-and-the-power-of-platforms/) - 
1)”Aggregators tend to internalize their network effects and commoditize their suppliers, which is exactly what Amazon has done.” 2) “A platform alternative — a company that succeeds by enabling its suppliers to differentiate and externalizing network effects to create a mutually beneficial ecosystem.” “Shopify is a platform: instead of interfacing with customers directly, 820k 3P merchants (2019) sit on top of Shopify and are responsible for acquiring all of those customers on their own.Amazon is pursuing customers and bringing suppliers and merchants onto its platform on its own terms; Shopify is giving merchants an opportunity to differentiate themselves while bearing no risk if they fail.merchants: interfacing with all of them on an individual basis is not scalable for those 3PL companies; now, though, they only need to interface with Shopify.”The same benefit applies in the opposite direction: merchants don’t have the means to negotiate with multiple 3PLs such that their inventory is optimally placed to offer fast and inexpensive delivery to customers; worse, the small-scale sellers I discussed above often can’t even get an audience with these logistics companies. Now, though, Shopify customers need only interface with Shopify.” 3)”Shopify has already accomplished this when it comes to referral partners (who drive new merchants onto the platform), developers (who build apps for managing Shopify stores) and theme designers (who sell themes to customize the look-and-feel of stores) — compete with each other narrowly and ensure that Shopify succeeds broadly”

## 9. ECommerce

## 10. Statistics

## 11. Big Picture

11.1. [Disrupting Disruption](https://every.to/divinations/disrupting-disruption-734208) - A thought provoking challenge to Clayton Christensen's Disruption Theory. I liked the use of concrete examples such as Uber, iPhone, Costco, Walmart, Southwest Airlines.

11.2. [Complexity Convection](https://every.to/divinations/complexity-convection-765851) - This article stayed with me for its insights on the value of keeping products and businesses simple and the pressures and incentives that work against it.

11.3. [Publishers and the Smiling Curve](https://stratechery.com/2014/publishers-smiling-curve/) - This is where I first learned about the Smiling Curve. Ben Thompson's application of the framework to the publishing industry is an eye-opener, as it makes you consider the commonalities between seemingly disconnected industries.

11.4. [What Strategy Questions are You Asking](https://rogermartin.medium.com/what-strategy-questions-are-you-asking-587d32239a48) - I appreciated this article for highlighting that "moat" is dynamic and ever-changing. The true advantage lies in being ahead enough to ask and answer questions that competitors are not yet in a position to think about. It also asserts that a data advantage is really a questions advantage and that data analysis without a hypothesis is just data mining. This really resonated with me because speaking from experience, it's easy to fool ourselves and others with numbers and charts, mistaking complexity or effort for meaning. But that is ultimately noise. The framing—the questions—is the most important thing.

11.5. [Retention is Hard, and Getting Harder — Here’s Why](https://www.reforge.com/blog/retention-is-hard-and-getting-harder) - Offers a very clear view through the use of market maps how every vertical has been getting fiercely competitive with time. Mentions channel fatigue: "You may be a standout copywriter and you may come up with the the most engaging notifications and emails out there, but that won’t matter.Your job will be harder, simply by virtue of operating in the same channel as spammy notification and email pushers who exacerbate and accelerate fatigue.Companies with infrequent use cases — think any OTA and most retailers, like Orbitz, Pottery Barn or Ski.com — are prime examples of these players that overload people with email in an attempt to stay top of mind or manufacture demand.". Another problem contributing to channel fatigure: "we haven’t seen any groundbreaking new channels emerge in the last few years". The article mentions that companies with distribution power/money "either push upstarts out of the market before their products have a chance to take hold with customers, or buy them before they can reach their full potential". I would be interested to look into who has been successful in breaking through this structural challenge and how. An interesting point in this article is that in today's landscape, it's less about "growing the pie" than "splitting the pie": since you have more competition, your product likely won’t be meeting an unmet need or filling a void for your user. So, instead of wiring a new habit, your product must replace an existing habit.Finally, I found this statistic around retention interesting: According to 2015 mobile data from Quettra, Facebook retains 98% of users 90 days after install, Whatsapp retains 77%, and Instagram retains 48%, whereas Snap retains only 33%. It makes the case that Snap has been less successful in protecting itself from Facebook compared to Pinterest, eBay, Craigslist, GoFundMe and YouTube. My thinking is that this has to do with first-mover advtange in brand building. The other companies had more firmly established themselves for their specific use cases in people's mind; whereas Snap came in later and also seemed to be playing the coolness/exclusivity game than the mass-market game by intentionally being trickier to use. So not only did Snap come into the market late but structurally it was poised to capture fewer users, giving Facebook an advantage in keeping its distribution. Ultimately the network effect won and Snap lost its users when the novelty of it wore off.

## 12. Forecasting










 










